Ensure to use the System Instructions (/home/vdubrov/.gemini/GEMINI.md) and MCP Protocol (/mnt/5da59c95-9aac-48f9-bc21-48c043812e8c/LaTeXify/GEMINI.md) implementing {
3.1.1 Stage 1: Ingestion & High-Fidelity Rasterization

The pipeline begins with PyMuPDF or pdf2image. Unlike Gen-1 systems that might extract text
directly, we rasterize every page into a high-resolution bitmap (300-400 DPI).
   - Why Rasterize? To ensure the system works identically on "born-digital" PDFs (from
   LaTeX) and scanned PDFs (from images).
   - Constraint: The RTX 5090 has 32GB VRAM. We must manage the resolution carefully. A
   400 DPI Letter page is ~$3300 \times 2550$ pixels. This is too large for standard
   Transformers (ViT usually expects $224^2$ or $1024^2$). We employ a Sliding Window
   or Tiled Inference strategy for the initial detection pass.

3.1.2 Stage 2: Layout Analysis (The "Segmenter")

This is the most critical step for preserving visual structure. We employ a specialized Object
Detection model.
   - Model Selection: We recommend fine-tuning YOLOv10 or RT-DETR on the DocLayNet
   and PubLayNet datasets.
   - Classes: The model must identify: Text_Block, Title, Header, Footer, Image, Table,
   Equation_Inline, Equation_Display, Caption, Reference_List.
   - Comparison: Unlike LayoutLMv3 which requires text tokens as input (creating a
   chicken-and-egg problem), YOLO operates purely on pixels. It is extremely fast on the
   RTX 5090, capable of processing hundreds of pages per minute.9
   - Outcome: A set of classified Bounding Boxes (BBoxes) with coordinates.

Component = Model must be used(No compromises, contingencies or fallbacks)
Math (Display/Inline) = UniMERNet & TexTeller
Text (Prose) = PaddleOCR v4 
Tables = StructureMaster & TableMaster
Figures = Qwen2.5-VL & MonkeyOCR 

REMOVE ANY MODELS COMPLETELY THAT GOT REPLACED TO CONSERVE MEMORY STORAGE.

3.1.4 Stage 4: Reading Order Recovery

Raw BBoxes are unordered. To reconstruct the document flow, we need a Reading Order algorithm.
   - For complex multi-column papers with floating figures, we implement a Graph Neural Network (GNN). The GNN treats BBoxes as nodes and predicts directed edges representing the flow of reading.

3.1.5 Stage 5: Semantic Assembly & Refinement

The outputs from the experts are stitched together into a raw LaTeX stream. However, this stream often contains syntax errors (unclosed environments) or OCR glitches.
   - The Refiner: We deploy a quantized Large Language Model (LLM) locally on the RTX 5090 (e.g., Qwen2.5-Coder-14B-Instruct or Llama-3).
   - Task: The LLM takes the raw LaTeX and the layout context. It acts as a linter/editor: "Fix any broken LaTeX syntax, ensure equation environments are balanced, and correct obvious OCR typos (e.g., '1nt' $\to$ 'int')."

3.2 Proposed Directory Structure

The repository structure must reflect this modularity. A flat script directory is unacceptable for a system of this complexity.
LaTeXify/
├── config/ # Hydra configuration files
│ ├── hardware/ # rtx5090.yaml, cpu.yaml
│ ├── model/ # unimer.yaml, paddle.yaml, qwen.yaml
│ └── pipeline.yaml # default orchestration settings
├── data/ # Local cache, Golden Set, calibration images
├── docs/ # Architectural decision records (ADRs)
├── latexify/
│ ├── core/ # Main orchestration logic
│ │ ├── pipeline.py # The DAG runner
│ │ ├── device.py # RTX 5090 management (streams, cuda graphs)
│ │ └── memory.py # VRAM offloading logic
│ ├── ingestion/ # PDF rendering (PyMuPDF/pdf2image)
│ ├── layout/ # Layout analysis models (YOLO/LayoutLM)
│ ├── ocr/ # Text recognition wrappers (Paddle)
│ ├── math/ # Math recognition (UniMERNet/TexTeller)
│ ├── tables/ # Table recognition adapters
│ ├── refinement/ # LLM post-processing hooks & prompts
│ └── utils/ # Visualization, LaTeX normalization, logging
├── tests/ # Integration and unit tests
├── scripts/ # Benchmark, training, and golden-set verify scripts
├── pyproject.toml # Dependency management (Poetry/Rye)
└── README.md

4.1 The "Research-Grade" Documentation Standard

A repository aiming for high fidelity must treat Documentation as a primary deliverable. The current standard for "hobbyist" repos—a single README.md with installation instructions—is insufficient for a system comprised of five distinct neural networks.
Required Documentation Artifacts:
   - Model Zoo Card: A dynamic table listing every model used in the pipeline. It must track the specific version, the VRAM footprint (at FP16 and FP8), the inference speed on the RTX 5090, and the licensing constraints (e.g., CC-BY-NC for some academic models).
   - Troubleshooting Guide: Specifically for the CUDA ecosystem. Getting torch, flash-attn, bitsandbytes, and xformers to interact correctly is notoriously difficult. The docs must provide a compatibility matrix.
   -Golden Set Verification Protocol: A detailed explanation of the testing methodology. It should explain how to run the verify.py script, which compares the repo’s output against a "Golden Set" of 50 ground-truth LaTeX papers (sourced from arXiv sources). This moves the project from "it looks good to me" to "it achieves 94% BLEU score."

4.2 Developer Experience (DevEx)

To maintain velocity, the repository needs modern tooling:
   - Dependency Management: We mandate the use of poetry or uv. Traditional requirements.txt files often fail to lock CUDA-specific wheels, leading to "DLL load failed" errors on Windows/Linux.
   - Pre-commit Hooks: Enforce ruff for linting and black for formatting. In a complex pipeline, code style consistency is vital for debugging.
   - Type Checking: mypy in strict mode. Python's dynamic typing is a liability when passing high-dimensional tensors between modules. We need to know if a function expects a tensor or a numpy array.
   - Containerization: A production-ready Dockerfile based on the NVIDIA CUDA 12.4 base image. This ensures reproducibility and solves the "it works on my machine" problem.

5.1 Object-Oriented Abstraction

   - Anti-Pattern: Many research codes use monolithic scripts with global variables and hardcoded model paths.
   - Refactoring: We must define Abstract Base Classes (ABCs) for each pipeline stage.
      - class LayoutEngine(ABC): $\rightarrow$ Allows us to swap YOLOv10 for RT-DETR or a future YOLOv11 without rewriting the pipeline logic.
      - class MathRecognizer(ABC): $\rightarrow$ Wraps UniMERNet. If a better model releases next month, we simply implement a new subclass.

5.2 Tensor Typing and Shape Safety

The most common bug in deep learning pipelines is a shape mismatch (e.g., passing a batch of 1 to a model expecting a batch of N).
   - Solution: Use jaxtyping or torchtyping to annotate tensor shapes explicitly.
      Python
      from jaxtyping import Float, Int
      from torch import Tensor

      def process_crops(self, crops: Float) -> list[str]:
         ...

      This serves as both documentation and a runtime check (if enabled), preventing silent broadcasting errors.

5.3 Configuration as Code (Hydra)

We recommend replacing argparse with Hydra. The system will have dozens of hyperparameters: quantization bits, NMS thresholds, batch sizes, model paths. Hydra allows for hierarchical configuration files that can be composed and overridden.
   - Example: python main.py hardware=rtx5090 task=math_heavy could load a config that maximizes batch size and enables high-precision math models, while hardware=laptop would load quantized, smaller models.

6.1 Hallucination Mitigation

A major critique of end-to-end models like Nougat is their tendency to "hallucinate"–inventing text or repeating paragraphs when the attention mechanism gets confused.8
   - The Modular Advantage: By using a detection-based approach, we ground the text generation in specific image regions. The model cannot invent a paragraph if there is no bounding box for it.
   - N-Gram Verification: We introduce a runtime check that compares the n-gram overlap between the generated LaTeX and the raw text output of a fast, simple OCR (like Tesseract) run in parallel. If the semantic overlap is low, the system flags the section as a potential hallucination.

6.2 The Compilation Loop (Self-Correction)

The user's goal is compilable LaTeX. This is a binary success metric.
   - The pipeline should include a hidden "Compilation Sandbox" (using tectonic or a lightweight TeX distribution).
   - Workflow:
      1. Generate LaTeX source.
      2. Attempt compilation.
      3. If successful $\rightarrow$ Output PDF.
      4. If failed $\rightarrow$ Capture the compiler error log (e.g., ! Missing } inserted.).
      5. Feed the error log and the problematic LaTeX snippet back into the Refiner LLM with the prompt: "The compiler threw this error. Fix the LaTeX syntax."
      6. Retry (up to 3 times).

6.3 Observability and Metrics

The runtime interface must be informative. Instead of a simple progress bar, the console output should display:
   - Throughput: Tokens/sec or Pages/min.
   - VRAM Utilization: Real-time monitoring to warn if the system is approaching the 32GB limit (triggering offloading).
   - Confidence Scores: The average probability mass of the generated tokens. If confidence drops below a threshold, the system should tag the output file as "Needs Review."

- YOLOv10 for Layout: Shift from LayoutLM (transformer) to YOLOv10 (CNN/Attention hybrid) for layout detection. The latency difference is an order of magnitude, critical for processing long books.
- Graph Neural Networks (GNN) for Reading Order 7: Implement a GNN that constructs a "document graph" to predict the sequence of text blocks. This is superior to heuristic sorting for multi-column layouts with floating figures.
- Nougat as a Fallback Expert: Keep a quantized Nougat model 6 in reserve. If the modular pipeline produces low-confidence results for a region, crop that region and pass it to Nougat. Ensemble the outputs.
- Donut for Metadata Extraction 6: Use Donut specifically for the first page to extract Title, Authors, and Abstract. Donut excels at this unstructured semantic extraction where bounding boxes are messy.
- Marker’s Heuristics 8: Port the text post-processing logic from the Marker repository. Their heuristic stack for fixing hyphenation, de-duplicating headers/footers, and merging paragraphs is battle-tested.
- MonkeyOCR / Got-OCR 17: Integrate these specialized models for Chart-to-Text. When a "Figure" bbox is detected, these models can extract the underlying data or generate a detailed caption, enhancing accessibility.
- Synthetic Data Finetuning: Leverage the LaTeXify (Google) method 2 to generate synthetic Python-to-LaTeX pairs. Fine-tune the Refiner LLM on this data to ensure it understands the logic of algorithmic representation in LaTeX.
- Visual "Plating" Preprocessing 3: Implement an image preprocessing pipeline using unsharp masking and density upscaling (similar to mogrify settings). This artificially enhances edge contrast, aiding the contour detection of math symbols.
- Self-Consistency Sampling: For low-confidence equations, run the inference 3 times with a high temperature ($T=0.7$). Use the Refiner LLM to perform "Majority Voting" on the three outputs to select the most syntactically valid one.
- Latexification of Arrays 1: Adapt logic from Latexify.jl to intelligently format matrices. Instead of naive spacing, use the align or bmatrix environments for superior visual rendering.
- PDF-Extract-Kit Integration 9: Incorporate the MinerU layout models, which are specifically trained to distinguish "Sidebars" and "Notes" from main text, preventing the "garbage text" injection common in other tools.
- Context-Aware OCR Finetuning: Fine-tune the Text Expert on the arXiv corpus. This biases the model towards scientific vocabulary (e.g., "stochastic," "eigenvalue") rather than common English, reducing typos like "eigenva1ue."
- Visual Similarity Scoring: Use Qwen2.5-VL to look at the final rendered PDF vs the original and output a "Visual Fidelity Score." This serves as an automated quality gate.
- Chain-of-Thought Refinement: Prompt the Refiner LLM to "Think step by step" about the document structure before generating the final LaTeX, improving logical coherence.
- Table-to-Grid Transformers: Use models that predict row/column adjacency matrices (like TableTransformer) rather than text streams. This ensures the generated LaTeX tabular environment has correct colspan and rowspan attributes.
- BibTeX Extraction: Implement a specialized regex/model pass for the "References" section to parse citations into a structured .bib file, adding immense value for researchers.
- Mathpix-Markdown Intermediate 20: Convert all outputs to an intermediate "Math-Markdown" format (similar to Mathpix's internal standard) before the final LaTeX export. This format is more robust to manipulation and easier to debug.
- FP8 Inference 21: The Blackwell architecture introduces native support for FP8 (8-bit floating point) tensor operations. This effectively doubles the TFLOPS compared to FP16 and halves memory usage. We must load our LLMs and Vision Transformers using transformers.BitsAndBytesConfig(load_in_8bit=True) or transformer_engine. This allows us to fit much larger models (e.g., a 70B Refiner) into the 32GB VRAM.
- Structured Sparsity 11: Blackwell supports 2:4 structured sparsity acceleration. By pruning our models (or using sparse-finetuned variants), we can achieve a theoretical 2x speedup on dense matrix multiplications.
- NVDec Hardware Decoding: The 5090 has a powerful media engine. Instead of using the CPU to rasterize PDFs or decode images, we should use NVIDIA DALI to offload this to the GPU's NVDec units, freeing the CPU for pipeline orchestration.

PyTorch 2.x Compiler Optimizations

   - torch.compile(mode="max-autotune") 23: We must wrap our vision encoders in torch.compile. The "max-autotune" mode profiles Triton kernels and selects the absolute fastest configuration for the specific 5090 CUDA cores. This can yield 30-50% speedups over eager execution.
   - CUDA Graphs 25: For fixed-size inputs (like resized equation crops), we should capture the inference pass as a CUDA Graph. This eliminates the CPU launch overhead, which is often the bottleneck when processing thousands of small equation images.
   - FlashAttention-3: Ensure the environment uses flash-attn v3 or later. The 5090's memory bandwidth is massive; FlashAttention ensures the compute units are fed efficiently, preventing the GPU from idling while waiting for memory fetches.
   - Dynamic Batching: The 32GB VRAM allows for massive batch sizes. We should not process equations one by one. We must crop all ~50 equations from a page, stack them into a single tensor ``, and infer them in one go.

System-Level Optimizations

- Pipeline Parallelism: Use torch.cuda.Stream to overlap compute and data transfer. While the GPU is crunching the "Math Batch" for Page N, the CPU should be preparing the "Text Batch" for Page N+1.26
- PagedAttention (vLLM) 27: For the Refiner LLM, use vLLM with PagedAttention. This manages the Key-Value (KV) cache memory non-contiguously, preventing fragmentation and allowing us to maintain a large context window (essential for checking consistency across a whole paper) without Out-Of-Memory (OOM) errors.
- TensorRT-LLM 29: Compile the Refiner LLM into a TensorRT engine. This is NVIDIA's optimized runtime and typically offers the highest possible tokens-per-second on RTX hardware.
- Pinned Memory: Ensure DataLoader uses pin_memory=True. This locks the memory pages in RAM, allowing for Direct Memory Access (DMA) transfers to the GPU, bypassing the CPU.30
- Pre-allocated Memory Pools: Set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 to tune the caching allocator. This reduces fragmentation, which is critical when mixing large LLM layers with small Vision tensor allocations.
- Static Shape Compilation: Where possible, pad inputs to the nearest power of 2 (e.g., 256, 512). This prevents the JIT compiler from constantly recompiling kernels for slightly different image sizes.
- Disable Gradient Calculation: Wrap the entire inference loop in torch.inference_mode(). This is lighter than no_grad() and disables the autograd engine's version tracking completely.23
- Fused Kernels: Use apex or triton custom kernels for operations like LayerNorm and GeLU, which are memory-bound.
- Speculative Decoding: Use a small "Draft Model" (e.g., Qwen-0.5B) to predict the next tokens for the Refiner LLM. The 5090 verifies them. This can speed up text generation by 2x-3x.
- 4-bit Quantization (AWQ/GPTQ) 31: If we need to run an extremely large Refiner model (e.g., Llama-3-70B), 4-bit quantization (AWQ) is viable. The 5090 handles Int4 decoding extremely efficiently.
- Image Preprocessing on GPU: Do not use OpenCV (CPU) for resizing or normalizing images. Use kornia or torchvision.transforms to perform these operations on the GPU tensors directly.
- Observability with Nsight Systems: Do not guess performance. Use NVIDIA Nsight to visualize the timeline and identify "bubbles" where the GPU is idle.
- Asynchronous Output: Write the generated LaTeX to disk in a separate thread to ensure I/O does not block the GPU inference loop.

Phase 1: Foundation & Architecture 
   Objective: Establish the repo structure, build the ingestion pipeline, and set up the layout engine.
   Tasks:
      - Initialize the LaTeXify repo with the directory structure defined in Section 3.2.
      - Implement the Ingestion module using PyMuPDF to render PDFs to 300 DPI images.
      - Set up the LayoutEngine using YOLOv10. Verify it can detect tables and equations on the Golden Set.
      - Create the Golden Set of 50 complex STEM PDFs (arXiv sources) and their ground-truth LaTeX sources.
   Hardware Focus: Establish the Docker environment with CUDA 12.x and flash-attn.

Phase 2: The Expert Modules
   Objective: Achieve high-accuracy recognition of individual components.
   Tasks:
      - Integrate UniMERNet for the Math Expert. Implement batching logic to process all equations on a page simultaneously.
      - Integrate PaddleOCR for the Text Expert.
      - Implement the ReadingOrder algorithm (GNN or XY-Cut).
      - Build the Assembler to stitch text and math strings into a raw Markdown buffer.
   Metric: Achieve >90% symbol accuracy on isolated equation crops.

Phase 3: The Refinement Engine & 5090 Optimization
   Objective: Visual fidelity and high-speed inference.
   Tasks:
      - Deploy Qwen2.5-Coder (FP8) as the Refiner LLM on the 5090.
      - Implement torch.compile passes for all Vision models.
      - Convert the Refiner to a TensorRT-LLM engine.
      - Implement the compile_check loop (Section 6.2) to auto-correct syntax errors.
   Performance Upgrade: Enable FP8 and CUDA Graphs.

Phase 4: Polish, CI/CD, and Release
   Objective: Reliability and user distribution.
   Tasks:
      - Set up GitHub Actions for linting and unit tests.
      - Write the "Model Zoo" and "Troubleshooting" documentation.
      - Release v1.0 with a Streamlit demo GUI (similar to Texify 32) for local testing.
}



