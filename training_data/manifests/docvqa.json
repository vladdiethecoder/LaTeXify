{
  "name": "DocVQA",
  "slug": "docvqa",
  "tracks": [
    "T6"
  ],
  "url": "https://www.docvqa.org/datasets",
  "license": "Unknown",
  "storage": {
    "raw_root": "training_data/raw/docvqa",
    "processed_root": "training_data/processed/docvqa",
    "modalities": [
      {
        "modality": "page-image",
        "path": "training_data/raw/docvqa/images"
      },
      {
        "modality": "json",
        "path": "training_data/raw/docvqa/annotations"
      }
    ]
  },
  "ingest": {
    "strategy": "manual",
    "command": null,
    "needs_auth": true,
    "notes": [
      "Manual download required",
      "Visit https://www.docvqa.org/datasets and follow the provider instructions."
    ]
  },
  "preprocess": [
    "Utilize provided word/line-level OCR JSON",
    "Rasterize at 300 DPI if simulating from source PDFs"
  ],
  "strengths": [
    "Real-world document images",
    "50k QA pairs for training",
    "Includes OCR outputs"
  ],
  "limitations": [
    "License needs verification",
    "Focuses on QA, not structural extraction"
  ],
  "modality": [
    "page-image",
    "json"
  ],
  "domain": [
    "forms",
    "reports"
  ],
  "is_synthetic": false,
  "languages": [
    "en"
  ],
  "size": {
    "images": 12700,
    "pages": 12700,
    "pdfs": null,
    "qa_pairs": 50000
  },
  "relevance": 0.85,
  "justification": "Standard benchmark for Document Visual Question Answering, directly applicable to training key-info extraction models (T6)."
}
