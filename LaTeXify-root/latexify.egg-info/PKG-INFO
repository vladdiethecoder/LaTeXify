Metadata-Version: 2.4
Name: latexify
Version: 0.1.0
Summary: LaTeX document processing and synthesis toolkit
Author: LaTeXify Team
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pypdf>=3.9.0
Requires-Dist: typer>=0.12.5
Requires-Dist: pydantic>=2.7.0
Provides-Extra: faiss
Requires-Dist: faiss-cpu>=1.7.4; extra == "faiss"
Provides-Extra: ocr
Requires-Dist: easyocr>=1.7.1; extra == "ocr"
Requires-Dist: pytesseract>=0.3.10; extra == "ocr"
Requires-Dist: opencv-python>=4.8.0; extra == "ocr"
Provides-Extra: llama
Requires-Dist: llama-cpp-python>=0.2.90; extra == "llama"
Provides-Extra: hf-runner
Requires-Dist: transformers>=4.37.2; extra == "hf-runner"
Requires-Dist: accelerate>=0.28.0; extra == "hf-runner"
Requires-Dist: pillow>=10.3.0; extra == "hf-runner"
Requires-Dist: torch>=2.1.0; extra == "hf-runner"
Provides-Extra: dev
Requires-Dist: pytest>=8.2.0; extra == "dev"
Requires-Dist: ruff>=0.6.3; extra == "dev"
Requires-Dist: pre-commit>=3.7.0; extra == "dev"
Requires-Dist: aiohttp>=3.9.5; extra == "dev"
Requires-Dist: chktex>=1.7.9; python_version < "0" and extra == "dev"

# LaTeXify

LaTeXify is a modular, multi-agent pipeline that ingests PDFs (and other structured sources), plans the document structure, retrieves LaTeX knowledge, synthesizes snippets, and compiles a polished `.tex` + PDF deliverable. Each stage is implemented as a composable CLI so you can run the full pipeline or invoke individual agents.

## Quick Links
- `docs/SCHEMAS.md` – JSON schemas for plans, consensus blocks, and layout blueprints.
- `docs/tasks/` – prompts and task specifications for planner/retrieval/synthesis agents.
- `configs/` – Hydra-compatible config files (e.g., specialist router weights).
- `docs/HF_RUNNER.md` – Hugging Face Accelerate runner install/offload guide for InternVL.
- `HOW_TO_INSTALL.md` / `HOW_TO_RUN.md` – deep dives into environment provisioning and sample runs.

## Installation
1. Create an isolated environment and install the project directly from the pyproject definition:
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install -e '.[dev,ocr,llama]'
   ```
   - Use `pip install -e .` for the core pipeline only.
   - Extras: `ocr` pulls GPU/vision dependencies, `hf-runner` adds Transformers/Accelerate/Pillow/Torch for the InternVL HF runner, `llama` enables local LlamaCpp synthesis, `dev` installs pytest/ruff/etc.
2. Ensure a full TeX toolchain (`latexmk`, `pdflatex`, `chktex`) is available on your PATH. On macOS: `brew install --cask mactex && brew install chktex`; on Linux: install via TeXLive (`tlmgr install latexmk chktex`).
3. Optional models/backends:
   - OCR backends expect credentials or local endpoints; see `HOW_TO_INSTALL.md`.
   - Retrieval requires the KB artifacts under `kb/`; use `python -m latexify.pipeline.build_latex_kb` after editing KB JSONL files.
   - Hugging Face InternVL runner: after downloading weights, run `pip install -e '.[hf-runner]'` and follow `docs/HF_RUNNER.md` for device_map/`max_memory` guidance plus offload notes.

### Vision OCR Endpoints
High-quality ingestion relies on InternVL and Florence-2 chat endpoints. Configure them via environment variables or CLI flags:
```bash
export LATEXIFY_INTERNVL_ENDPOINT="http://localhost:8080/v1"
export LATEXIFY_FLORENCE_ENDPOINT="http://localhost:8081/v1"
```
Both endpoints must implement the OpenAI-compatible `/v1/chat/completions` contract and accept messages with text + image parts. If you are serving the bundled weights locally via vLLM, a typical launch is:
```bash
python -m vllm.entrypoints.openai.api_server \
  --model $MODELS/ocr/internvl-3.5-14b \
  --port 8080

python -m vllm.entrypoints.openai.api_server \
  --model $MODELS/ocr/florence-2-large \
  --port 8081
```
You can also plug in hosted endpoints from Azure/OpenAI-compatible providers—just point the env vars at their `/v1` base URL. CLI entry points accept `--internvl-endpoint` / `--florence-endpoint` to override per-run, and `--allow-fallback` forces the older heuristic extractor if you need to run without vision models.

### InternVL HF Runner Mode
If you want to keep InternVL inference in a standalone Accelerate process (device_map auto, CPU offload, process isolation), switch the ingestion council to “HF” mode:
```bash
pip install -e '.[hf-runner]'
python scripts/install_models.py internvl
INTERNVL_MODE=hf \
INTERNVL_HF_RUNNER=scripts/run_internvl_hf.py \
INTERNVL_HF_MAX_MEMORY="cuda:0=21GiB,cuda:1=21GiB,cpu=16GiB" \
INTERNVL_HF_GPUS="0,1" \
python -m latexify.ingestion.ingest_pdf --pdf inputs/doc.pdf --run-dir dev/runs/doc
```
- Use `configs/ingestion/internvl.yaml` (or set `INTERNVL_CONFIG`) to store persistent settings such as runner script path, `device_map`, `max_memory`, offload folder, timeout, and retry policy. See `docs/HF_RUNNER.md` for a full reference.
- To revert to the legacy vLLM HTTP path, either unset `INTERNVL_MODE` or set it to `vllm`; the ingestion adapter automatically falls back when HF settings are disabled.

### One-Click Environment Setup
Prefer an opinionated bootstrapper? Run `scripts/setup_one_click.py` to install Python dependencies, download the default models, launch local vLLM servers for InternVL/Florence, run the HF runner dry-run, and execute both a pipeline smoke test (`run_local.py`) plus selected pytest suites. Example:
```bash
python scripts/setup_one_click.py \
  --pdf "Basic Skills Review Unit USER.pdf" \
  --internvl-port 8090 --florence-port 8091
```
- Pass `--skip-vllm` (or `--skip-pytest`, `--skip-smoke`, etc.) if you are on a CPU-only host.
- The script auto-detects/install a supported Python (3.10–3.13). If you launch it from Python 3.14, it will re-exec under `python3.12`/`python3.13` when available (override via `--python-bin`). It also creates/uses a `.venv` (change via `--venv-path`), writes logs under `build/setup_logs/`, and tears down background processes on exit.

## Quickstart CLI
- Run the full ingestion→plan→synthesis→aggregation line in one step:
  ```bash
  latexify run inputs/doc.pdf --out-dir build --title "My Lesson"
  ```
  This command creates `build/runs/<doc-stem>/`, extracts OCR (including figures/tables), auto-generates a plan from the judge consensus, writes snippets, runs QA, and compiles `main.tex`/`main.pdf`.
- Still need individual stages? The Typer gateway lets you forward arguments: e.g. `latexify ingest -- --pdf doc.pdf --run-dir dev/runs/doc`.
- Dev sandbox runner for bundled fixtures (`dev/inputs/*.pdf`):
  ```bash
  python run_local.py --pdf all --title "Skills Review"
  ```
  Processes every PDF under `dev/inputs/` (rubric/assessment/user responses) sequentially, placing per-file outputs under `dev/runs/<stem>/` and `build/<stem>/`.
  If OCR endpoints are not configured the script will warn and temporarily enable the heuristic fallback so the run still completes; export the endpoint env vars above for best quality. `run_local.py` automatically re-executes itself inside the project virtualenv (defaults to `.venv`, override via `LATEXIFY_VENV_PATH`) and ensures a supported Python (3.10–3.13) so you always use the same dependency stack prepared by `setup_one_click.py`. Set `LATEXIFY_PYTHON_BIN=/usr/bin/python3.12` if you need to force a particular interpreter.

### Fast Start Checklist
1. `python -m venv .venv && source .venv/bin/activate`
2. `pip install -e '.[ocr]'` (add `,llama` if you need local LlamaCpp) and ensure `latexmk` + `chktex` are installed from TeX Live.
3. Export `LATEXIFY_INTERNVL_ENDPOINT` & `LATEXIFY_FLORENCE_ENDPOINT` or pass `--internvl-endpoint/--florence-endpoint`.
4. Run `python run_local.py --pdf all --title "Skills Review"` to process every sample PDF and generate textbook-style PDFs under `build/<stem>/`.

## Pipeline Stages & CLI Entry Points
| Stage | Description | Primary Module / CLI |
| --- | --- | --- |
| Layout Blueprint | Converts rough notes / sample pages into `layout_plan.json` (doc class hints, sections, figure slots). | `python -m latexify.pipeline.layout_planner --text-file draft.txt --out build/layout_plan.json`
| Ingestion Council | Runs pdf chunking + multi-backend OCR (`InternVL`, `Florence`, `MinerU`, `Nougat`) and emits consensus snippets + assets. | `python -m latexify.ingestion.ingest_pdf --pdf input.pdf --run-dir dev/runs/sample`
| Planner Scaffold | Builds the canonical `plan.json` (tasks, ordering, asset links) using OCR consensus + layout hints. | `python -m latexify.pipeline.planner_scaffold --plan build/plan.json --layout build/layout_plan.json`
| Retrieval Bundles | Enriches each plan task with KB context (packages, examples) stored under `bundles/`. | `python -m latexify.pipeline.retrieval_bundle --plan build/plan.json --out build/bundles`
| Specialist Synthesis | Routes every bundle to the right specialist (text/table/math/figure) and generates snippets+metadata. | `python -m latexify.pipeline.synth_latex --bundles build/bundles --plan build/plan.json --out build/snippets`
| QA & Validator | Static + compile-time QA (`chktex`, env fixes, critic retries) before aggregation. | `python -m latexify.pipeline.qa_validator --plan build/plan.json --snippets build/snippets --attempt-compile`
| Aggregation & Compile | Writes `main.tex`, copies assets, and runs `latexmk` / auto-fix loop to produce the PDF. | `python scripts/pipeline.py --seed 1337` or `python -m latexify.pipeline.aggregator ...`

You can run everything via `scripts/pipeline.py` (which chains planner → retrieval → synthesis → aggregation) or call each stage independently for debugging.

## Configuration via Hydra (`configs/`)
We use Hydra-like conventions for tunable components. Configuration files live under `configs/` (e.g., `configs/router.yaml` for the specialist router). To override values:
```bash
python -m latexify.pipeline.synth_latex --config-dir configs --config-name router \
  router.weights.figure=1.2 router.tag_overrides.task_ids.FIGX=figure
```
Common configuration surfaces:
- **Router weights:** adjust specialist biasing without editing code.
- **Planner/synthesis flags:** pass `hydra.run.dir` overrides when running experimentation via Hydra.
- **Model endpoints:** Several agents honour environment variables (e.g., `INTERNVL_ENDPOINT`) plus Hydra overrides.

## JSON Schemas & Artifacts
All cross-stage payloads conform to the schemas documented in `docs/SCHEMAS.md`:
- `PlanSchema` (`build/plan.json`)
- `ConsensusBlockSchema` (`run_dir/consensus/*.jsonl`)
- `LayoutBlueprintSchema` (planner output)
When editing agents, validate against these schemas (PyDantic models live in `latexify/models/schemas.py`).

## Repository Layout
```
latexify/
  ingestion/           # OCR council + chunkers + orchestrator
  pipeline/            # Planner, retrieval, synthesis, QA, aggregator
  assembly/            # Fallback LaTeX snippet renderers & utilities
  utils/               # Logging, doc-class helpers, dependency checks
configs/               # Hydra YAML files
kb/                    # Knowledge-base inputs & build logs
build/                 # Generated plans/snippets/artifacts (gitignored)
docs/                  # Developer notes, schemas, prompt specs
dev/                   # Fixtures, sample runs, evaluation helpers
tests/                 # Pytest suites covering ingestion, planner, QA, etc.
```

## Usage Examples
1. **Single-command end-to-end run:**
   ```bash
   latexify run inputs/doc.pdf --out-dir build --doc-class lix_textbook
   ```
2. **Manual stage-by-stage (advanced debugging):**
   ```bash
   python -m latexify.ingestion.ingest_pdf --pdf inputs/doc.pdf --run-dir dev/runs/doc
   python -m latexify.pipeline.planner_scaffold --plan build/plan.json --layout dev/runs/doc/consensus/manifest.json
   python -m latexify.pipeline.synth_latex --plan build/plan.json --bundles build/bundles --out build/snippets
   python -m latexify.pipeline.aggregator --plan build/plan.json --snippets build/snippets --out build --compile
   ```
3. **Legacy scripts orchestrator:** `python scripts/pipeline.py --seed 4242`
4. **QA-only pass on existing snippets:**
   ```bash
   python -m latexify.pipeline.qa_validator --plan build/plan.json --snippets build/snippets --attempt-compile
   ```

## Testing & Linting
- Run the full suite: `pytest -q`. Targeted modules live under `tests/` (e.g., `tests/test_tex_assembler.py`).
- Static QA depends on `chktex`; install it to enable validator tests.
- Optional: run `ruff check .` (if you installed the `dev` extra) before submitting patches.

## Support & Troubleshooting
- `docs/DEV_NOTES.md` – current developer road map and architectural notes.
- `docs/SCHEMAS.md` – keep your agent outputs compliant.
- `evidence/` – QA/preflight logs from previous runs.
- File an issue or PR with logs (`build/qa/preflight-compile.log`) when reporting problems.
